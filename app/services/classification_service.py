# íŒŒì¼ ìœ„ì¹˜: app/services/classification_service.py
# ì„¤ëª…: ì†¡ê¸ˆì„ ë³„ë„ ë¶„ë¥˜í•˜ëŠ” ê³ ê¸‰ AI ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤
# í˜‘ì—… ê°€ì´ë“œ: 
# 1. ì†¡ê¸ˆ ê°ì§€ í›„ ë³„ë„ ë¶„ë¥˜ (ì‚¬ëŒ ì´ë¦„ + ì†¡ê¸ˆ ì„œë¹„ìŠ¤)
# 2. ì†¡ê¸ˆì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê¸°ì¡´ AI ë¶„ë¥˜ ë¡œì§ ì‹¤í–‰
# 3. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ "ì†¡ê¸ˆ" ì¹´í…Œê³ ë¦¬ëŠ” ì†Œë¹„ ë¹„ìœ¨ ê³„ì‚°ì—ì„œ ì œì™¸
# 4. ì†¡ê¸ˆ ì •ë³´ëŠ” ë³„ë„ ì˜ì—­ì— í‘œì‹œ

from typing import List, Dict, Any, Optional
from datetime import datetime
import pandas as pd
import re
import random
import numpy as np
from difflib import get_close_matches
import logging

# ì„ íƒì  import (ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘)
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    BERT_AVAILABLE = True
    print("âœ… BERT ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    BERT_AVAILABLE = False
    print("âš ï¸ BERT ëª¨ë¸ ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘")

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
    print("âœ… KoNLPy í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPy ë¯¸ì„¤ì¹˜, í˜•íƒœì†Œ ë¶„ì„ ë¹„í™œì„±í™”")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.decomposition import TruncatedSVD
    from sklearn.preprocessing import LabelEncoder
    from sklearn.model_selection import train_test_split, RandomizedSearchCV
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score
    from scipy import sparse
    from scipy.sparse import csr_matrix
    SKLEARN_AVAILABLE = True
    print("âœ… scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ë¶„ë¥˜ë§Œ ì‚¬ìš©")

class TransactionClassificationService:
    """
    ì†¡ê¸ˆì„ ë³„ë„ ë¶„ë¥˜í•˜ëŠ” ê³ ê¸‰ AI ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤
    (ì†¡ê¸ˆ ë¶„ë¦¬ + ìš°ìˆ˜í•œ ë¶„ë¥˜ ì„±ëŠ¥ + í”„ë¡œë•ì…˜ ì•ˆì •ì„±)
    """
    def __init__(self):
        # ğŸ”¥ ì†¡ê¸ˆ ê°ì§€ íŒ¨í„´ ì •ì˜
        self.REMITTANCE_PATTERNS = {
            "person_name_patterns": [
                r'^[ê°€-í£]{2,3}$',  # 2-3ê¸€ì í•œê¸€ ì´ë¦„
                r'^[ê°€-í£]{2,4}ë‹˜$',  # ì´ë¦„ + ë‹˜
                r'^[ê°€-í£]{2,4}\s*ì”¨$',  # ì´ë¦„ + ì”¨
            ],
            "remittance_services": [
                "í† ìŠ¤í˜ì´", "ì¹´ì¹´ì˜¤í˜ì´", "í˜ì´ì½”", "ë„¤ì´ë²„í˜ì´", "ì‚¼ì„±í˜ì´",
                "ì†¡ê¸ˆ", "ì´ì²´", "ì…ê¸ˆ", "ìš©ëˆ", "ê³„ì¢Œì´ì²´", "ë¬´í†µì¥ì…ê¸ˆ",
                "ì˜¨ë¼ì¸ì´ì²´", "ATMì´ì²´", "ëª¨ë°”ì¼ì†¡ê¸ˆ", "ê°„í¸ì†¡ê¸ˆ"
            ],
            "exclude_keywords": [
                # ì†¡ê¸ˆìœ¼ë¡œ ì˜¤ì¸ë  ìˆ˜ ìˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ì¼ë°˜ ì†Œë¹„ì¸ ê²ƒë“¤
                "ë§ˆíŠ¸", "í¸ì˜ì ", "ì¹´í˜", "ìŒì‹ì ", "ë³‘ì›", "ì•½êµ­",
                "ì£¼ìœ ì†Œ", "ëŒ€í•™êµ", "í•™ì›", "íšŒì‚¬", "ìƒì ", "ë§¤ì¥", "ì "
            ]
        }
        
        # ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬ ì¶”ê°€)
        self.CATEGORIES: Dict[str, List[str]] = {
            "ì£¼ê±°ë¹„":        ["í•œêµ­ì „ë ¥ê³µì‚¬","ì„œìš¸ë„ì‹œê°€ìŠ¤","í•œêµ­ìˆ˜ë ¥ì›ìë ¥","LHí† ì§€ì£¼íƒê³µì‚¬","ì„œìš¸ì£¼íƒë„ì‹œê³µì‚¬","ì˜ˆìŠ¤ì½”","í•œì „"],
            "êµí†µë¹„":        ["ì¹´ì¹´ì˜¤íƒì‹œ","Të§µíƒì‹œ","ê³ ì†ë²„ìŠ¤","SRT","ì½”ë ˆì¼","ì§€í•˜ì² "],
            "í†µì‹ ë¹„":        ["SKT","KT","LGU+","ì•Œëœ°í°ìƒµ","ë°ì´í„°ì¶©ì „","LGU+ì¸í„°ë„·"],
            "ì˜ë£Œë¹„":        ["ì„œìš¸ëŒ€ë³‘ì›","ì‚¼ì„±ì„œìš¸ë³‘ì›","ì•„ì‚°ë³‘ì›","ê°•ë‚¨ì—°ì„¸ì¹˜ê³¼","ë©”ë””íí”¼ë¶€ê³¼"],
            "êµìœ¡ë¹„":        ["YBMì–´í•™ì›","ë©”ê°€ìŠ¤í„°ë””","í•´ì»¤ìŠ¤ì–´í•™ì›","ì—ë“€ìœŒ","í•´ì»¤ìŠ¤íŒ¨ìŠ¤","í”„ë¦°íŠ¸","CHATGPT"],
            "ì‹ë¹„":          ["ë§¥ë„ë‚ ë“œ","ìŠ¤íƒ€ë²…ìŠ¤","ì´ë””ì•¼ì»¤í”¼","ë°°ë‹¬ì˜ë¯¼ì¡±","ìš”ê¸°ìš”","ì»´í¬ì¦ˆì»¤í”¼","ì–´ì˜¤ë„¤","ì˜¹ê¸°ì¢…ê¸°","ë¹½ë‹¤ë°©","ë°˜ì "],
            "ìƒí™œìš©í’ˆë¹„":    ["GS25","CU","ì´ë§ˆíŠ¸","ë¡¯ë°ë§ˆíŠ¸","í™ˆí”ŒëŸ¬ìŠ¤","ì„¸ë¸ì¼ë ˆë¸"],
            "ì´ë¯¸ìš©/ì˜ë¥˜/í™”ì¥í’ˆ": ["ë¬´ì‹ ì‚¬","H&M","ZARA","ì•„ëª¨ë ˆí¼ì‹œí”½","ì—ë›°ë“œí•˜ìš°ìŠ¤","TEMU"],
            "ì˜¨ë¼ì¸ ì»¨í…ì¸ ":  ["ë„·í”Œë¦­ìŠ¤","ì™“ì± ","ìœ íŠœë¸Œí”„ë¦¬ë¯¸ì—„","ë©œë¡ ","ì¿ íŒ¡í”Œë ˆì´","SOOP"],
            "ì—¬ê°€ë¹„":        ["CGV","ë¡¯ë°ì‹œë„¤ë§ˆ","ì„œìš¸ëœë“œ","ë¡¯ë°ì›”ë“œ","ìŠ¤íƒ€í•„ë“œ","ë…¸ë˜","PC"],
            "ê¸°íƒ€":          ["11ë²ˆê°€","ì¿ íŒ¡","ì˜¥ì…˜","Gë§ˆì¼“","ì¸í„°íŒŒí¬"],
            "ì†¡ê¸ˆ":          []  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€ëœ ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬
        }

        # ë£° ê¸°ë°˜ í‚¤ì›Œë“œ (ìš°ì„ ìˆœìœ„)
        self.RULES: List[tuple] = [
            ("í†µì‹ ë¹„",   ["í†µì‹ ","ì¸í„°ë„·","LGU+","SKT","KT"]),
            ("ì‹ë¹„",     ["ë°°ë¯¼","ìš”ê¸°ìš”","ë§¥ë„ë‚ ë“œ","ìŠ¤íƒ€ë²…ìŠ¤","ì»¤í”¼","ì´ë””ì•¼"]),
            ("êµí†µë¹„",   ["íƒì‹œ","ë²„ìŠ¤","ì§€í•˜ì² ","SRT","ì½”ë ˆì¼"]),
            ("ì˜ë£Œë¹„",   ["ë³‘ì›","ì¹˜ê³¼","ì•½êµ­","í”¼ë¶€ê³¼"]),
        ]

        # í¼ì§€ ë§¤ì¹­ìš© ìƒ˜í”Œ
        self.CATEGORY_MAP: Dict[str, List[str]] = {cat: samples[:] for cat, samples in self.CATEGORIES.items()}
        self.ALL_SAMPLES: List[str] = [s for samples in self.CATEGORY_MAP.values() for s in samples if samples]  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì œì™¸
        
        # íŒŒë¼ë¯¸í„°
        self.DESIRED_PER_CAT = 1000
        self.AUG_FACTOR = 1
        self.TFIDF_DIM = 50
        self.MORPH_DIM = 5
        self.BATCH_SIZE = 128
        self.RANDOM_STATE = 42
        self.RSEARCH_ITERS = 4
        self.RSEARCH_CV = 2
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.model_initialized = False
        self.ai_model_available = False
        
        # ëª¨ë¸ ë³€ìˆ˜ë“¤ (Noneìœ¼ë¡œ ì´ˆê¸°í™”)
        self.tokenizer = None
        self.bert_model = None
        self.tfidf_vectorizer = None
        self.svd_tfidf = None
        self.count_vectorizer = None
        self.svd_morph = None
        self.okt = None
        self.label_encoder = None
        self.ml_classifier = None
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        print("ğŸš€ ì†¡ê¸ˆ ë¶„ë¦¬ ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def _is_remittance(self, name: str) -> bool:
        """
        ì†¡ê¸ˆ ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜
        
        Args:
            name: ì •ê·œí™”ëœ ê±°ë˜ì²˜ëª…
            
        Returns:
            bool: ì†¡ê¸ˆì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not name or not name.strip():
            return False
            
        name_lower = name.lower().strip()
        
        # 1. ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (ì¼ë°˜ ì†Œë¹„ë¡œ í™•ì‹¤í•œ ê²ƒë“¤)
        for exclude_keyword in self.REMITTANCE_PATTERNS["exclude_keywords"]:
            if exclude_keyword in name_lower:
                return False
        
        # 2. ì‚¬ëŒ ì´ë¦„ íŒ¨í„´ ì²´í¬
        for pattern in self.REMITTANCE_PATTERNS["person_name_patterns"]:
            if re.match(pattern, name):
                return True
        
        # 3. ì†¡ê¸ˆ ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ ì²´í¬
        for service in self.REMITTANCE_PATTERNS["remittance_services"]:
            if service.lower() in name_lower:
                return True
                
        return False

    def initialize_model(self):
        """ì§€ì—° ì´ˆê¸°í™” + ëœë¤ ì‹œë“œ ê³ ì •"""
        if self.model_initialized:
            return
            
        try:
            # ëœë¤ ì‹œë“œ ê³ ì •
            random.seed(self.RANDOM_STATE)
            np.random.seed(self.RANDOM_STATE)
            
            print("ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self.model_initialized = True
            print("âœ… ê¸°ë³¸ ë¶„ë¥˜ ì™„ë£Œ")
            
            # AI ëª¨ë¸ë“¤ì´ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ AI ëª¨ë“œ í™œì„±í™”
            if BERT_AVAILABLE and SKLEARN_AVAILABLE:
                print("ğŸ”¥ AI ëª¨ë¸ ë¡œë”© ë° í•™ìŠµ ì‹œì‘")
                self._initialize_ai_models()
                self.ai_model_available = True
                print("âœ… AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            else:
                print("âš ï¸ AI íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘")
                
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘")

    def _initialize_ai_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            if BERT_AVAILABLE:
                self.tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
                self.bert_model = AutoModel.from_pretrained("monologg/koelectra-small-v3-discriminator").eval()
                
            if KONLPY_AVAILABLE:
                self.okt = Okt()
                
            self._train_ai_models()
            
        except Exception as e:
            self.logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_model_available = False
            raise

    def _train_ai_models(self):
        """AI ëª¨ë¸ í•™ìŠµ"""
        try:
            # í•™ìŠµ ë°ì´í„° ìƒì„± (ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬ ì œì™¸)
            training_data = self._generate_training_data()
            docs, labels = zip(*training_data)
            
            # í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ
            self._train_feature_extractors(docs, labels)
            
            # ML ëª¨ë¸ í•™ìŠµ
            self._train_ml_model(docs, labels)
            
        except Exception as e:
            self.logger.error(f"AI ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            self.ai_model_available = False
            raise

    def _generate_training_data(self):
        """í•™ìŠµ ë°ì´í„° ìƒì„± (ì¦ê°• í¬í•¨) - ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬ ì œì™¸"""
        try:
            # ê¸°ë³¸ ë°ì´í„° ìƒì„± (ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸)
            base = []
            for cat, samples in self.CATEGORIES.items():
                if cat == "ì†¡ê¸ˆ" or not samples:  # ì†¡ê¸ˆ ì¹´í…Œê³ ë¦¬ì™€ ë¹ˆ ìƒ˜í”Œì€ ì œì™¸
                    continue
                    
                for i in range(self.DESIRED_PER_CAT):
                    base_sample = samples[i % len(samples)]
                    suffix = i // len(samples) + 1
                    name = f"{base_sample}{suffix:03d}" if suffix > 1 else base_sample
                    base.append((name, cat))

            # ë°ì´í„° ì¦ê°•
            augmented = []
            for name, cat in base:
                normalized = self.normalize(name)
                augmented.append((normalized, cat))
                
                # ì¶”ê°€ ì¦ê°•
                for _ in range(self.AUG_FACTOR):
                    augmented_name = self._augment_name(name)
                    augmented.append((self.normalize(augmented_name), cat))
                    
            return augmented
            
        except Exception as e:
            self.logger.error(f"í•™ìŠµ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _augment_name(self, name: str) -> str:
        """ë°ì´í„° ì¦ê°•"""
        try:
            # ê³µë°± ì‚½ì…
            if random.random() < 0.3 and len(name) > 1:
                pos = random.randint(1, len(name) - 1)
                name = name[:pos] + " " + name[pos:]
                
            # ì–¸ë”ìŠ¤ì½”ì–´ ë³€í™˜
            if random.random() < 0.2 and " " in name:
                name = name.replace(" ", "_", 1)
                
            # "ì " ì¶”ê°€
            if random.random() < 0.2:
                name = name + "ì "
                
            # ë³„í‘œ ë³€í™˜
            if random.random() < 0.1 and " " in name:
                name = name.replace(" ", "*", 1)
                
            return name
            
        except Exception as e:
            self.logger.warning(f"ë°ì´í„° ì¦ê°• ì¤‘ ì˜¤ë¥˜: {e}")
            return name

    def _train_feature_extractors(self, docs: List[str], labels: List[str]):
        """í”¼ì²˜ ì¶”ì¶œê¸°ë“¤ í•™ìŠµ"""
        if not SKLEARN_AVAILABLE:
            return
            
        try:
            # TF-IDF + SVD
            self.tfidf_vectorizer = TfidfVectorizer(
                analyzer="char", 
                ngram_range=(2, 4), 
                min_df=5
            )
            X_tfidf = self.tfidf_vectorizer.fit_transform(docs)
            self.svd_tfidf = TruncatedSVD(
                n_components=self.TFIDF_DIM, 
                random_state=self.RANDOM_STATE
            )
            self.svd_tfidf.fit(X_tfidf)
            
            # í˜•íƒœì†Œ ë¶„ì„ + SVD (KoNLPy ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if KONLPY_AVAILABLE and self.okt:
                tokens = [" ".join(self.okt.nouns(doc)) for doc in docs]
                self.count_vectorizer = CountVectorizer(min_df=5)
                X_morph = self.count_vectorizer.fit_transform(tokens)
                self.svd_morph = TruncatedSVD(
                    n_components=self.MORPH_DIM, 
                    random_state=self.RANDOM_STATE
                )
                self.svd_morph.fit(X_morph)
                
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise

    def _train_ml_model(self, docs: List[str], labels: List[str]):
        """ML ëª¨ë¸ í•™ìŠµ"""
        if not SKLEARN_AVAILABLE:
            return
            
        try:
            # í”¼ì²˜ ì¶”ì¶œ
            X = self._extract_features_batch(docs)
            
            # ë¼ë²¨ ì¸ì½”ë”©
            self.label_encoder = LabelEncoder().fit(labels)
            y = self.label_encoder.transform(labels)
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, stratify=y, random_state=self.RANDOM_STATE
            )
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
            search = RandomizedSearchCV(
                RandomForestClassifier(random_state=self.RANDOM_STATE),
                {
                    "n_estimators": [100, 200],
                    "max_depth": [None, 10],
                    "min_samples_leaf": [1, 2]
                },
                n_iter=self.RSEARCH_ITERS,
                cv=self.RSEARCH_CV,
                n_jobs=-1,
                random_state=self.RANDOM_STATE
            )
            
            search.fit(X_train, y_train)
            self.ml_classifier = search.best_estimator_
            
            # ì„±ëŠ¥ í‰ê°€
            y_pred = self.ml_classifier.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"ğŸ¯ ëª¨ë¸ ì •í™•ë„: {accuracy:.3f}")
            
        except Exception as e:
            self.logger.error(f"ML ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            self.ai_model_available = False
            raise

    def _extract_features_batch(self, texts: List[str]):
        """ë°°ì¹˜ í”¼ì²˜ ì¶”ì¶œ"""
        try:
            features = []
            
            # BERT í”¼ì²˜
            if BERT_AVAILABLE and self.bert_model and self.tokenizer:
                bert_features = self._batch_bert(texts)
                features.append(csr_matrix(bert_features))
            
            # TF-IDF í”¼ì²˜
            if self.tfidf_vectorizer and self.svd_tfidf:
                tfidf_features = self.tfidf_vectorizer.transform(texts)
                tfidf_reduced = self.svd_tfidf.transform(tfidf_features)
                features.append(csr_matrix(tfidf_reduced))
            
            # í˜•íƒœì†Œ í”¼ì²˜
            if KONLPY_AVAILABLE and self.okt and self.count_vectorizer and self.svd_morph:
                tokens = [" ".join(self.okt.nouns(text)) for text in texts]
                morph_features = self.count_vectorizer.transform(tokens)
                morph_reduced = self.svd_morph.transform(morph_features)
                features.append(csr_matrix(morph_reduced))
            
            # ìˆ˜ì¹˜í˜• í”¼ì²˜
            lengths = np.array([len(text) for text in texts])[:, None]
            digit_counts = np.array([sum(c.isdigit() for c in text) for text in texts])[:, None]
            numeric_features = np.hstack([lengths, digit_counts])
            features.append(csr_matrix(numeric_features))
            
            # ëª¨ë“  í”¼ì²˜ ê²°í•©
            if features:
                return sparse.hstack(features)
            else:
                # í”¼ì²˜ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ìˆ˜ì¹˜í˜•ë§Œ ë°˜í™˜
                return csr_matrix(numeric_features)
                
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ í”¼ì²˜ë¼ë„ ë°˜í™˜
            lengths = np.array([len(text) for text in texts])[:, None]
            return csr_matrix(lengths)

    def _batch_bert(self, texts: List[str]):
        """ë°°ì¹˜ BERT ì¸ì½”ë”©"""
        try:
            vectors = []
            for i in range(0, len(texts), self.BATCH_SIZE):
                batch = texts[i:i + self.BATCH_SIZE]
                encoded = self.tokenizer(
                    batch,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=32
                )
                
                with torch.no_grad():
                    outputs = self.bert_model(**encoded)
                    
                batch_vectors = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                vectors.append(batch_vectors)
                
            return np.vstack(vectors)
            
        except Exception as e:
            self.logger.error(f"BERT ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë²¡í„° ë°˜í™˜
            return np.zeros((len(texts), 768))

    def normalize(self, name: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not name or not isinstance(name, str):
            return ""
            
        try:
            normalized = name.lower().replace("_", " ").replace("*", " ")
            normalized = re.sub(r"(ì $|\d{3}$)", "", normalized)
            return normalized.strip()
            
        except Exception as e:
            self.logger.warning(f"ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return str(name).strip()

    def rule_based_category(self, name: str) -> Optional[str]:
        """ë£° ê¸°ë°˜ ë¶„ë¥˜"""
        try:
            for category, keywords in self.RULES:
                for keyword in keywords:
                    if keyword.lower() in name.lower():
                        return category
            return None
            
        except Exception as e:
            self.logger.warning(f"ë£° ê¸°ë°˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return None

    def fuzzy_category(self, name: str, threshold: int = 90) -> Optional[str]:
        """í¼ì§€ ë§¤ì¹­ ë¶„ë¥˜"""
        try:
            matches = get_close_matches(
                name, 
                self.ALL_SAMPLES, 
                n=1, 
                cutoff=threshold/100
            )
            
            if matches:
                match = matches[0]
                for category, samples in self.CATEGORY_MAP.items():
                    if match in samples:
                        return category
                        
            return None
            
        except Exception as e:
            self.logger.warning(f"í¼ì§€ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return None

    def categorize_store(self, name: str) -> str:
        """
        ğŸ”¥ ì†¡ê¸ˆ ë¶„ë¦¬ + í–¥ìƒëœ ë¶„ë¥˜ í•¨ìˆ˜
        ìˆœì„œ: ì†¡ê¸ˆ ì²´í¬ â†’ ë£° ê¸°ë°˜ â†’ í¼ì§€ ë§¤ì¹­ â†’ AI ëª¨ë¸
        """
        # ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸
        if not self.model_initialized:
            self.initialize_model()
            
        # ì…ë ¥ ê²€ì¦
        if not name or not name.strip():
            return "ê¸°íƒ€"
            
        try:
            normalized = self.normalize(name)
            
            # ğŸ”¥ 1ë‹¨ê³„: ì†¡ê¸ˆ ì—¬ë¶€ ë¨¼ì € ì²´í¬
            if self._is_remittance(normalized):
                return "ì†¡ê¸ˆ"

            # 2ë‹¨ê³„: ë£° ê¸°ë°˜ ë¶„ë¥˜
            rule_result = self.rule_based_category(normalized)
            if rule_result:
                return rule_result

            # 3ë‹¨ê³„: í¼ì§€ ë§¤ì¹­
            fuzzy_result = self.fuzzy_category(normalized, threshold=90)
            if fuzzy_result:
                return fuzzy_result

            # 4ë‹¨ê³„: AI ëª¨ë¸ ë¶„ë¥˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if self.ai_model_available and self.ml_classifier and self.label_encoder:
                try:
                    features = self._extract_features_batch([normalized])
                    prediction = self.ml_classifier.predict(features)[0]
                    category = self.label_encoder.inverse_transform([prediction])[0]
                    return category
                    
                except Exception as e:
                    self.logger.warning(f"AI ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨, í´ë°± ë¶„ë¥˜ë¡œ ì „í™˜: {e}")

            # 5ë‹¨ê³„: ìµœì¢… í´ë°±: ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­
            for category, samples in self.CATEGORIES.items():
                if category == "ì†¡ê¸ˆ":  # ì†¡ê¸ˆì€ ì´ë¯¸ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì œì™¸
                    continue
                for sample in samples:
                    if sample.lower() in normalized or normalized in sample.lower():
                        return category

            # 6ë‹¨ê³„: ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ
            return "ê¸°íƒ€"
            
        except Exception as e:
            self.logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return "ê¸°íƒ€"

    def parse_nh_excel(self, file_path: str) -> List[Dict[str, Any]]:
        """NHë†í˜‘ ì—‘ì…€ íŒŒì¼ íŒŒì‹± (ì•ˆì •ì„± ê°•í™”)"""
        if not self.model_initialized:
            self.initialize_model()
            
        try:
            # íŒŒì¼ ì½ê¸°
            raw_df = pd.read_excel(file_path, header=None)
            
            # í—¤ë” ì°¾ê¸°
            header_rows = raw_df[raw_df.apply(
                lambda row: row.astype(str).str.contains("ìˆœë²ˆ").any(), axis=1
            )].index
            
            if len(header_rows) == 0:
                raise ValueError("'ìˆœë²ˆ' í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            # ì‹¤ì œ ë°ì´í„° ì½ê¸°
            df = pd.read_excel(file_path, header=header_rows[0])
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì°¾ê¸°
            required_columns = {
                'ê±°ë˜ê¸°ë¡ì‚¬í•­': None,
                'ì¶œê¸ˆê¸ˆì•¡': None,
                'ê±°ë˜ì¼ì‹œ': None
            }
            
            for col in df.columns:
                col_str = str(col)
                for req_col in required_columns:
                    if req_col in col_str and required_columns[req_col] is None:
                        required_columns[req_col] = col
                        
            # ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
            missing_columns = [k for k, v in required_columns.items() if v is None]
            if missing_columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = df[df[required_columns['ì¶œê¸ˆê¸ˆì•¡']].notna()]
            df = df[df[required_columns['ì¶œê¸ˆê¸ˆì•¡']].astype(str).str.strip() != ""]
            df = df[df[required_columns['ì¶œê¸ˆê¸ˆì•¡']].astype(str).str.strip() != "0"]
            
            # ë¶„ë¥˜ ìˆ˜í–‰
            store_names = df[required_columns['ê±°ë˜ê¸°ë¡ì‚¬í•­']].astype(str).str.strip().tolist()
            categories = [self.categorize_store(name) for name in store_names]
            
            # ê²°ê³¼ êµ¬ì„±
            result = []
            for i, (_, row) in enumerate(df.iterrows()):
                try:
                    # ê¸ˆì•¡ íŒŒì‹±
                    amount_str = str(row[required_columns['ì¶œê¸ˆê¸ˆì•¡']]).strip().replace(",", "")
                    amount = float(re.sub(r"[^\d.]", "", amount_str))
                    
                    if amount <= 0:
                        continue
                        
                    result.append({
                        "transaction_date": row.get(required_columns['ê±°ë˜ì¼ì‹œ'], datetime.now()),
                        "amount": amount,
                        "store_name": store_names[i],
                        "category": categories[i],
                        "description": store_names[i],
                        "payment_method": "ì¹´ë“œ"
                    })
                    
                except (ValueError, TypeError) as e:
                    self.logger.warning(f"í–‰ {i} ì²˜ë¦¬ ì¤‘ ìŠ¤í‚µ: {e}")
                    continue
                    
            return result
            
        except Exception as e:
            self.logger.error(f"NH ì—‘ì…€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise

    def parse_csv_file(self, file_path: str) -> List[Dict[str, Any]]:
        """CSV íŒŒì¼ íŒŒì‹± (ì•ˆì •ì„± ê°•í™”)"""
        if not self.model_initialized:
            self.initialize_model()
            
        try:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„
            df = None
            encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
                    
            if df is None:
                raise ValueError("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì»¬ëŸ¼ ìë™ ë§¤í•‘
            column_mapping = {
                'date': ['ë‚ ì§œ', 'ê±°ë˜ì¼', 'ì¼ì', 'date', 'ê±°ë˜ì¼ì‹œ'],
                'amount': ['ê¸ˆì•¡', 'ì¶œê¸ˆì•¡', 'ì…ê¸ˆì•¡', 'ì‚¬ìš©ê¸ˆì•¡', 'amount', 'ì¶œê¸ˆê¸ˆì•¡'],
                'merchant': ['ê°€ë§¹ì ', 'ìƒí˜¸ëª…', 'ì ìš”', 'merchant', 'ê±°ë˜ê¸°ë¡ì‚¬í•­']
            }
            
            found_columns = {}
            for target, candidates in column_mapping.items():
                for col in df.columns:
                    col_lower = str(col).lower()
                    for candidate in candidates:
                        if candidate.lower() in col_lower and target not in found_columns:
                            found_columns[target] = col
                            break
                    if target in found_columns:
                        break
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if 'amount' not in found_columns or 'merchant' not in found_columns:
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(ê¸ˆì•¡, ê°€ë§¹ì )ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„° ì²˜ë¦¬
            result = []
            valid_rows = []
            
            for idx, row in df.iterrows():
                try:
                    # ê¸ˆì•¡ ì²˜ë¦¬
                    amount_str = str(row[found_columns['amount']]).replace(',', '').strip()
                    if not amount_str or amount_str == 'nan':
                        continue
                        
                    amount = float(re.sub(r"[^\d.]", "", amount_str))
                    if amount <= 0:
                        continue
                    
                    # ê°€ë§¹ì ëª… ì²˜ë¦¬
                    merchant_name = str(row[found_columns['merchant']]).strip()
                    if not merchant_name or merchant_name == 'nan':
                        merchant_name = 'ê¸°íƒ€'
                    
                    valid_rows.append((idx, merchant_name, amount, row))
                    
                except (ValueError, TypeError):
                    continue
            
            # ë°°ì¹˜ ë¶„ë¥˜
            merchant_names = [name for _, name, _, _ in valid_rows]
            categories = [self.categorize_store(name) for name in merchant_names]
            
            # ê²°ê³¼ êµ¬ì„±
            for i, (idx, name, amount, row) in enumerate(valid_rows):
                transaction_date = datetime.now()
                if 'date' in found_columns:
                    try:
                        transaction_date = pd.to_datetime(row[found_columns['date']])
                    except:
                        pass
                
                result.append({
                    'transaction_date': transaction_date,
                    'amount': amount,
                    'store_name': name,
                    'category': categories[i],
                    'description': name,
                    'payment_method': 'ì¹´ë“œ'
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise

    def get_category_statistics(self, transactions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ğŸ”¥ ì†¡ê¸ˆ ë¶„ë¦¬ í†µê³„ ìƒì„±
        
        Args:
            transactions: ê±°ë˜ ë‚´ì—­ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì†¡ê¸ˆ ë¶„ë¦¬ëœ í†µê³„ ì •ë³´
        """
        try:
            if not transactions:
                return {
                    "consumption_analysis": {},
                    "remittance_info": {},
                    "total_transactions": 0
                }
            
            # ì†¡ê¸ˆê³¼ ì‹¤ì œ ì†Œë¹„ ë¶„ë¦¬
            remittance_transactions = [t for t in transactions if t.get('category') == 'ì†¡ê¸ˆ']
            consumption_transactions = [t for t in transactions if t.get('category') != 'ì†¡ê¸ˆ']
            
            # ì‹¤ì œ ì†Œë¹„ ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
            consumption_by_category = {}
            total_consumption = 0
            
            for transaction in consumption_transactions:
                category = transaction.get('category', 'ê¸°íƒ€')
                amount = float(transaction.get('amount', 0))
                
                if category not in consumption_by_category:
                    consumption_by_category[category] = 0
                consumption_by_category[category] += amount
                total_consumption += amount
            
            # ì‹¤ì œ ì†Œë¹„ ë¹„ìœ¨ ê³„ì‚° (ì†¡ê¸ˆ ì œì™¸)
            consumption_percentages = {}
            if total_consumption > 0:
                for category, amount in consumption_by_category.items():
                    consumption_percentages[category] = round((amount / total_consumption) * 100, 1)
            
            # ì†¡ê¸ˆ ì •ë³´ ì§‘ê³„
            total_remittance = sum(float(t.get('amount', 0)) for t in remittance_transactions)
            remittance_count = len(remittance_transactions)
            
            # ì „ì²´ ê±°ë˜ ëŒ€ë¹„ ì†¡ê¸ˆ ë¹„ìœ¨
            total_amount = sum(float(t.get('amount', 0)) for t in transactions)
            remittance_percentage = round((total_remittance / total_amount) * 100, 1) if total_amount > 0 else 0
            
            return {
                "consumption_analysis": {
                    "categories": consumption_by_category,
                    "percentages": consumption_percentages,
                    "total_amount": total_consumption,
                    "transaction_count": len(consumption_transactions)
                },
                "remittance_info": {
                    "total_amount": total_remittance,
                    "transaction_count": remittance_count,
                    "percentage_of_total": remittance_percentage,
                    "transactions": remittance_transactions[:10]  # ìµœê·¼ 10ê±´ë§Œ
                },
                "total_transactions": len(transactions),
                "summary": {
                    "consumption_total": total_consumption,
                    "remittance_total": total_remittance,
                    "grand_total": total_amount
                }
            }
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "consumption_analysis": {},
                "remittance_info": {},
                "total_transactions": 0,
                "error": str(e)
            }

    def analyze_file_and_classify(self, file_path: str, file_type: str = "auto") -> Dict[str, Any]:
        """
        íŒŒì¼ ë¶„ì„ ë° ë¶„ë¥˜ í†µí•© í•¨ìˆ˜
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            file_type: íŒŒì¼ íƒ€ì… ("excel", "csv", "auto")
            
        Returns:
            Dict: ë¶„ë¥˜ ê²°ê³¼ ë° í†µê³„
        """
        try:
            # íŒŒì¼ íƒ€ì… ìë™ ê°ì§€
            if file_type == "auto":
                if file_path.lower().endswith(('.xlsx', '.xls')):
                    file_type = "excel"
                elif file_path.lower().endswith('.csv'):
                    file_type = "csv"
                else:
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            
            # íŒŒì¼ íŒŒì‹±
            if file_type == "excel":
                transactions = self.parse_nh_excel(file_path)
            elif file_type == "csv":
                transactions = self.parse_csv_file(file_path)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…: {file_type}")
            
            # í†µê³„ ìƒì„±
            statistics = self.get_category_statistics(transactions)
            
            return {
                "success": True,
                "transactions": transactions,
                "statistics": statistics,
                "message": f"ì´ {len(transactions)}ê±´ì˜ ê±°ë˜ë¥¼ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "transactions": [],
                "statistics": {},
                "error": str(e),
                "message": "íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
classification_service = TransactionClassificationService()