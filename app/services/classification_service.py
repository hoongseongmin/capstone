# íŒŒì¼ ìœ„ì¹˜: app/services/classification_service.py
# ì„¤ëª…: ê³ ê¸‰ AI ê¸°ë²•ì´ í†µí•©ëœ ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤ (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´)

from typing import List, Dict, Any
from datetime import datetime
import pandas as pd
import re
import tempfile
import os
import random
import numpy as np
from difflib import get_close_matches

# ì„ íƒì  import (ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘)
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    BERT_AVAILABLE = True
    print("âœ… BERT ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    BERT_AVAILABLE = False
    print("âš ï¸ BERT ëª¨ë¸ ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘")

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
    print("âœ… KoNLPy í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPy ë¯¸ì„¤ì¹˜, í˜•íƒœì†Œ ë¶„ì„ ë¹„í™œì„±í™”")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.decomposition import TruncatedSVD
    from sklearn.preprocessing import LabelEncoder
    from sklearn.model_selection import train_test_split, RandomizedSearchCV
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, accuracy_score
    from scipy import sparse
    from scipy.sparse import csr_matrix
    SKLEARN_AVAILABLE = True
    print("âœ… scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ë¶„ë¥˜ë§Œ ì‚¬ìš©")


class TransactionClassificationService:
    """
    ê³ ê¸‰ AI ê¸°ë²•ì´ í†µí•©ëœ ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤
    
    ê¸°ì¡´ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ê³ ê¸‰ AI ë¶„ë¥˜ ê¸°ë²•ì„ ì¶”ê°€í•œ ë²„ì „
    - ê¸°ì¡´: ë£° ê¸°ë°˜ + ìƒ˜í”Œ ë§¤ì¹­ ë¶„ë¥˜
    - ì¶”ê°€: BERT + TF-IDF + í˜•íƒœì†Œ ë¶„ì„ + ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜
    """
    
    def __init__(self):
        # ========== ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì™„ì „ ë³´ì¡´) ==========
        self.CATEGORIES = {
            "ì£¼ê±°ë¹„":        ["í•œêµ­ì „ë ¥ê³µì‚¬","ì„œìš¸ë„ì‹œê°€ìŠ¤","í•œêµ­ìˆ˜ë ¥ì›ìë ¥","LHí† ì§€ì£¼íƒê³µì‚¬","ì„œìš¸ì£¼íƒë„ì‹œê³µì‚¬","ì˜ˆìŠ¤ì½”","í•œì „"],
            "êµí†µë¹„":        ["ì¹´ì¹´ì˜¤íƒì‹œ","Të§µíƒì‹œ","ê³ ì†ë²„ìŠ¤","SRT","ì½”ë ˆì¼","ì§€í•˜ì² "],
            "í†µì‹ ë¹„":        ["SKT","KT","LGU+","ì•Œëœ°í°ìƒµ","ë°ì´í„°ì¶©ì „","LGU+ì¸í„°ë„·"],
            "ì˜ë£Œë¹„":        ["ì„œìš¸ëŒ€ë³‘ì›","ì‚¼ì„±ì„œìš¸ë³‘ì›","ì•„ì‚°ë³‘ì›","ê°•ë‚¨ì—°ì„¸ì¹˜ê³¼","ë©”ë””íí”¼ë¶€ê³¼"],
            "êµìœ¡ë¹„":        ["YBMì–´í•™ì›","ë©”ê°€ìŠ¤í„°ë””","í•´ì»¤ìŠ¤ì–´í•™ì›","ì—ë“€ìœŒ","í•´ì»¤ìŠ¤íŒ¨ìŠ¤","í”„ë¦°íŠ¸","CHATGPT"],
            "ì‹ë¹„":          ["ë§¥ë„ë‚ ë“œ","ìŠ¤íƒ€ë²…ìŠ¤","ì´ë””ì•¼ì»¤í”¼","ë°°ë‹¬ì˜ë¯¼ì¡±","ìš”ê¸°ìš”","ì»´í¬ì¦ˆì»¤í”¼","ì–´ì˜¤ë„¤","ì˜¹ê¸°ì¢…ê¸°","ë¹½ë‹¤ë°©"],
            "ìƒí™œìš©í’ˆë¹„":    ["GS25","CU","ì´ë§ˆíŠ¸","ë¡¯ë°ë§ˆíŠ¸","í™ˆí”ŒëŸ¬ìŠ¤","ì„¸ë¸ì¼ë ˆë¸"],
            "ì´ë¯¸ìš©/ì˜ë¥˜/í™”ì¥í’ˆ": ["ë¬´ì‹ ì‚¬","H&M","ZARA","ì•„ëª¨ë ˆí¼ì‹œí”½","ì—ë›°ë“œí•˜ìš°ìŠ¤","TEMU"],
            "ì˜¨ë¼ì¸ ì»¨í…ì¸ ":  ["ë„·í”Œë¦­ìŠ¤","ì™“ì± ","ìœ íŠœë¸Œí”„ë¦¬ë¯¸ì—„","ë©œë¡ ","ì¿ íŒ¡í”Œë ˆì´","SOOP"],
            "ì—¬ê°€ë¹„":        ["CGV","ë¡¯ë°ì‹œë„¤ë§ˆ","ì„œìš¸ëœë“œ","ë¡¯ë°ì›”ë“œ","ìŠ¤íƒ€í•„ë“œ","ë…¸ë˜","PC"],
            "ê¸°íƒ€":          ["11ë²ˆê°€","ì¿ íŒ¡","ì˜¥ì…˜","Gë§ˆì¼“","ì¸í„°íŒŒí¬","í† ìŠ¤í˜ì´","ì¹´ì¹´ì˜¤í˜ì´"]
        }

        
        # ========== ê¸°ì¡´ ë£° ê¸°ë°˜ í‚¤ì›Œë“œ (ì™„ì „ ë³´ì¡´) ==========
        self.RULES = [
            ("í†µì‹ ë¹„", ["í†µì‹ ", "ì¸í„°ë„·", "LGU+", "SKT", "KT", "íœ´ëŒ€í°"]),
            ("ì‹ë¹„", ["ë°°ë¯¼", "ìš”ê¸°ìš”", "ë§¥ë„ë‚ ë“œ", "ìŠ¤íƒ€ë²…ìŠ¤", "ì»¤í”¼", "ì´ë””ì•¼", "ì¹˜í‚¨", "í”¼ì"]),
            ("êµí†µë¹„", ["íƒì‹œ", "ë²„ìŠ¤", "ì§€í•˜ì² ", "SRT", "ì½”ë ˆì¼", "ê³ ì†ë²„ìŠ¤"]),
            ("ì˜ë£Œë¹„", ["ë³‘ì›", "ì¹˜ê³¼", "ì•½êµ­", "í”¼ë¶€ê³¼", "í•œì˜ì›"]),
            ("ì£¼ê±°ë¹„", ["ì „ê¸°", "ê°€ìŠ¤", "ìˆ˜ë„", "ê´€ë¦¬ë¹„", "ì›”ì„¸"]),
            ("ìƒí™œìš©í’ˆë¹„", ["í¸ì˜ì ", "ë§ˆíŠ¸", "ì´ë§ˆíŠ¸", "ë¡¯ë°ë§ˆíŠ¸", "í™ˆí”ŒëŸ¬ìŠ¤"])
        ]
        
        # ========== ê³ ê¸‰ AI ê¸°ë²• ì¶”ê°€ ì„¤ì • ==========
        # í¼ì§€ ë§¤ì¹­ìš© ë°ì´í„°
        self.CATEGORY_MAP = {cat: samples[:] for cat, samples in self.CATEGORIES.items()}
        self.ALL_SAMPLES = [s for samples in self.CATEGORY_MAP.values() for s in samples]
        
        # AI ëª¨ë¸ ê´€ë ¨ ì„¤ì •
        self.DESIRED_PER_CAT = 1000
        self.AUG_FACTOR = 1
        self.TFIDF_DIM = 50
        self.MORPH_DIM = 5
        self.BATCH_SIZE = 128
        self.RANDOM_STATE = 42
        self.RSEARCH_ITERS = 4
        self.RSEARCH_CV = 2
        
        # ëª¨ë¸ ì´ˆê¸°í™” ìƒíƒœ
        self.model_initialized = False
        self.ai_model_available = False
        
        # AI ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜ë“¤
        self.tokenizer = None
        self.bert_model = None
        self.tfidf_vectorizer = None
        self.svd_tfidf = None
        self.count_vectorizer = None
        self.svd_morph = None
        self.okt = None
        self.label_encoder = None
        self.ml_classifier = None
        
        print("ğŸš€ ê³ ê¸‰ AI ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ê¸°ëŠ¥ + ê³ ê¸‰ AI ê¸°ëŠ¥)"""
        if self.model_initialized:
            return
            
        print("ğŸ¤– ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ê¸°ì¡´ ê°„ì†Œí™” ëª¨ë¸ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        self.model_initialized = True
        print("âœ… ê¸°ë³¸ ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ê³ ê¸‰ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œë„
        try:
            if BERT_AVAILABLE and SKLEARN_AVAILABLE:
                print("ğŸ”¥ ê³ ê¸‰ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
                self._initialize_ai_models()
                self.ai_model_available = True
                print("âœ… ê³ ê¸‰ AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            else:
                print("âš ï¸ ê³ ê¸‰ AI íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘")
                
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ ê¸°ë³¸ ë£° ê¸°ë°˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            self.ai_model_available = False
    
    def _initialize_ai_models(self):
        """ê³ ê¸‰ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        # 1. BERT ëª¨ë¸ ì´ˆê¸°í™”
        if BERT_AVAILABLE:
            print("ğŸ”¥ BERT ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
            self.bert_model = AutoModel.from_pretrained("monologg/koelectra-small-v3-discriminator").eval()
            print("âœ… BERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        if KONLPY_AVAILABLE:
            print("ğŸ”¥ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
            self.okt = Okt()
            print("âœ… í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 3. í›ˆë ¨ ë°ì´í„° ìƒì„± ë° ëª¨ë¸ í›ˆë ¨
        print("ğŸ”¥ í›ˆë ¨ ë°ì´í„° ìƒì„± ë° ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        self._train_ai_models()
        print("âœ… AI ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
    
    def _train_ai_models(self):
        """AI ëª¨ë¸ í›ˆë ¨"""
        try:
            # í›ˆë ¨ ë°ì´í„° ìƒì„±
            training_data = self._generate_training_data()
            docs = [item[0] for item in training_data]
            labels = [item[1] for item in training_data]
            
            # í”¼ì²˜ ì¶”ì¶œê¸° í›ˆë ¨
            self._train_feature_extractors(docs, labels)
            
            # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
            self._train_ml_model(docs, labels)
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            self.ai_model_available = False
    
    def _generate_training_data(self):
        """í›ˆë ¨ ë°ì´í„° ìƒì„±"""
        base = []
        for cat, samples in self.CATEGORIES.items():
            for i in range(self.DESIRED_PER_CAT):
                b = samples[i % len(samples)]
                suf = i // len(samples) + 1
                nm = f"{b}{suf:03d}" if suf > 1 else b
                base.append((nm, cat))
        
        # ë°ì´í„° ì¦ê°•
        aug = []
        for nm, cat in base:
            n0 = self.normalize(nm)
            aug.append((n0, cat))
            for _ in range(self.AUG_FACTOR):
                aug.append((self.normalize(self._augment_name(nm)), cat))
        
        return aug
    
    def _augment_name(self, name: str) -> str:
        """ì´ë¦„ ì¦ê°•"""
        if random.random() < 0.3 and len(name) > 1:
            p = random.randint(1, len(name) - 1)
            name = name[:p] + " " + name[p:]
        if random.random() < 0.2 and " " in name:
            name = name.replace(" ", "_", 1)
        if random.random() < 0.2:
            name = name + "ì "
        if random.random() < 0.1 and " " in name:
            name = name.replace(" ", "*", 1)
        return name
    
    def _train_feature_extractors(self, docs, labels):
        """í”¼ì²˜ ì¶”ì¶œê¸° í›ˆë ¨"""
        if not SKLEARN_AVAILABLE:
            return
            
        # TF-IDF + SVD
        self.tfidf_vectorizer = TfidfVectorizer(analyzer="char", ngram_range=(2, 4), min_df=5)
        X_tfidf = self.tfidf_vectorizer.fit_transform(docs)
        self.svd_tfidf = TruncatedSVD(n_components=self.TFIDF_DIM, random_state=self.RANDOM_STATE)
        self.svd_tfidf.fit(X_tfidf)
        
        # í˜•íƒœì†Œ ë¶„ì„ + SVD (KoNLPy ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if KONLPY_AVAILABLE and self.okt:
            tokens = [" ".join(self.okt.nouns(s)) for s in docs]
            self.count_vectorizer = CountVectorizer(min_df=5)
            X_morph = self.count_vectorizer.fit_transform(tokens)
            self.svd_morph = TruncatedSVD(n_components=self.MORPH_DIM, random_state=self.RANDOM_STATE)
            self.svd_morph.fit(X_morph)
    
    def _train_ml_model(self, docs, labels):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        if not SKLEARN_AVAILABLE:
            return
            
        try:
            # í”¼ì²˜ ì¶”ì¶œ
            X_all = self._extract_features_batch(docs)
            
            # ë¼ë²¨ ì¸ì½”ë”©
            self.label_encoder = LabelEncoder()
            y = self.label_encoder.fit_transform(labels)
            
            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_tr, X_te, y_tr, y_te = train_test_split(
                X_all, y, test_size=0.2, stratify=y, random_state=self.RANDOM_STATE
            )
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
            search = RandomizedSearchCV(
                RandomForestClassifier(random_state=self.RANDOM_STATE),
                {"n_estimators": [100, 200], "max_depth": [None, 10], "min_samples_leaf": [1, 2]},
                n_iter=self.RSEARCH_ITERS, cv=self.RSEARCH_CV, n_jobs=-1, verbose=0, random_state=self.RANDOM_STATE
            )
            search.fit(X_tr, y_tr)
            self.ml_classifier = search.best_estimator_
            
            # ì„±ëŠ¥ í‰ê°€
            y_pred = self.ml_classifier.predict(X_te)
            accuracy = accuracy_score(y_te, y_pred)
            print(f"ğŸ¯ AI ëª¨ë¸ ì •í™•ë„: {accuracy:.3f}")
            
        except Exception as e:
            print(f"âŒ ML ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            self.ml_classifier = None
    
    def _extract_features_batch(self, texts):
        """ë°°ì¹˜ í”¼ì²˜ ì¶”ì¶œ"""
        features = []
        
        # BERT í”¼ì²˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if BERT_AVAILABLE and self.tokenizer and self.bert_model:
            bert_features = self._batch_bert(texts)
            features.append(sparse.csr_matrix(bert_features))
        
        # TF-IDF í”¼ì²˜
        if self.tfidf_vectorizer and self.svd_tfidf:
            tfidf_features = self.svd_tfidf.transform(self.tfidf_vectorizer.transform(texts))
            features.append(sparse.csr_matrix(tfidf_features))
        
        # í˜•íƒœì†Œ í”¼ì²˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if KONLPY_AVAILABLE and self.okt and self.count_vectorizer and self.svd_morph:
            tokens = [" ".join(self.okt.nouns(s)) for s in texts]
            morph_features = self.svd_morph.transform(self.count_vectorizer.transform(tokens))
            features.append(sparse.csr_matrix(morph_features))
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜
        lengths = np.array([len(s) for s in texts])[:, None]
        digit_counts = np.array([sum(c.isdigit() for c in s) for s in texts])[:, None]
        num_features = np.hstack([lengths, digit_counts])
        features.append(sparse.csr_matrix(num_features))
        
        if features:
            return sparse.hstack(features)
        else:
            # ê¸°ë³¸ í”¼ì²˜ (ê¸¸ì´ë§Œ)
            return sparse.csr_matrix(lengths)
    
    def _batch_bert(self, texts):
        """ë°°ì¹˜ BERT ì„ë² ë”©"""
        vectors = []
        for i in range(0, len(texts), self.BATCH_SIZE):
            batch = texts[i:i + self.BATCH_SIZE]
            encoded = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=32)
            with torch.no_grad():
                output = self.bert_model(**encoded)
            vectors.append(output.last_hidden_state[:, 0, :].cpu().numpy())
        return np.vstack(vectors)
    
    def normalize(self, name: str) -> str:
        """ìƒí˜¸ëª… ì •ê·œí™” (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´)"""
        if not isinstance(name, str):
            name = str(name)
        
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        n = name.lower().replace("_", " ").replace("*", " ")
        # ì§€ì ëª…, ìˆ«ì ì œê±°
        n = re.sub(r"(ì $|\d{3}$)", "", n)
        return n.strip()
    
    def rule_based_category(self, name: str) -> str:
        """ë£° ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´)"""
        for cat, kws in self.RULES:
            for kw in kws:
                if kw.lower() in name.lower():
                    return cat
        return None
    
    def fuzzy_category(self, name: str, threshold: int = 90) -> str:
        """í¼ì§€ ë§¤ì¹­ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€)"""
        matches = get_close_matches(name, self.ALL_SAMPLES, n=1, cutoff=threshold/100)
        if matches:
            match = matches[0]
            for cat, samples in self.CATEGORY_MAP.items():
                if match in samples:
                    return cat
        return None
    
    def categorize_store(self, name: str) -> str:
        """
        ìƒí˜¸ëª… ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ê¸°ì¡´ + ê³ ê¸‰ AI í†µí•©)
        
        ë¶„ë¥˜ ìš°ì„ ìˆœìœ„:
        1. ê¸°ì¡´ ë£° ê¸°ë°˜ ë¶„ë¥˜ (ìµœìš°ì„ )
        2. í¼ì§€ ë§¤ì¹­ (ê³ ê¸‰ ê¸°ëŠ¥)
        3. AI ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ (ìµœê³ ê¸‰ ê¸°ëŠ¥)
        4. ê¸°ì¡´ ìƒ˜í”Œ ê¸°ë°˜ ë¶„ë¥˜ (í´ë°±)
        """
        if not name or str(name).strip() == '':
            return "ê¸°íƒ€"
            
        n = self.normalize(str(name))
        
        # 0) ì‚¬ëŒ ì´ë¦„ ê°ì§€ (í•œê¸€ 2-3ê¸€ì)
        if re.fullmatch(r'[ê°€-í£]{2,3}', n):
            return "ê¸°íƒ€"
        
        # 1) ê¸°ì¡´ ë£° ê¸°ë°˜ ë¶„ë¥˜ (ìµœìš°ì„ )
        rule_result = self.rule_based_category(n)
        if rule_result:
            return rule_result
        
        # 2) í¼ì§€ ë§¤ì¹­ (ê³ ê¸‰ ê¸°ëŠ¥)
        fuzzy_result = self.fuzzy_category(n, threshold=90)
        if fuzzy_result:
            return fuzzy_result
        
        # 3) AI ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ (ìµœê³ ê¸‰ ê¸°ëŠ¥, ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.ai_model_available and self.ml_classifier and self.label_encoder:
            try:
                features = self._extract_features_batch([n])
                prediction = self.ml_classifier.predict(features)[0]
                return self.label_encoder.inverse_transform([prediction])[0]
            except Exception as e:
                print(f"âš ï¸ AI ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©: {e}")
        
        # 4) ê¸°ì¡´ ìƒ˜í”Œ ê¸°ë°˜ ë¶„ë¥˜ (í´ë°±)
        for category, samples in self.CATEGORIES.items():
            for sample in samples:
                if sample.lower() in n or n in sample.lower():
                    return category
        
        return "ê¸°íƒ€"
    
    def parse_nh_excel(self, file_path: str) -> List[Dict[str, Any]]:
        """
        NHë†í˜‘ ê±°ë˜ë‚´ì—­ ì—‘ì…€ íŒŒì¼ íŒŒì‹± (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´)
        """
        # ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
        if not self.model_initialized:
            self.initialize_model()
            
        try:
            # ì—‘ì…€ íŒŒì¼ ì½ê¸°
            raw = pd.read_excel(file_path, header=None)
            
            # 'ìˆœë²ˆ' í—¤ë” ì°¾ê¸°
            hdr_rows = raw[raw.apply(lambda r: r.astype(str).str.contains("ìˆœë²ˆ").any(), axis=1)].index
            if len(hdr_rows) == 0:
                raise ValueError("'ìˆœë²ˆ' í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NHë†í˜‘ ê±°ë˜ë‚´ì—­ íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            # í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì½ê¸°
            df = pd.read_excel(file_path, header=hdr_rows[0])
            
            # í•„ìš”í•œ ì»¬ëŸ¼ ì°¾ê¸°
            want_cols = ["ê±°ë˜ì¼ì‹œ", "ì¶œê¸ˆê¸ˆì•¡", "ì…ê¸ˆê¸ˆì•¡", "ê±°ë˜í›„ì”ì•¡", "ê±°ë˜ë‚´ìš©", "ê±°ë˜ê¸°ë¡ì‚¬í•­"]
            cols = []
            for want in want_cols:
                found_col = None
                for col in df.columns:
                    if want in str(col):
                        found_col = col
                        break
                if found_col:
                    cols.append(found_col)
            
            if not cols:
                raise ValueError("í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            df = df[cols].fillna("")
            
            # ì¶œê¸ˆ ê±°ë˜ë§Œ í•„í„°ë§
            ì¶œê¸ˆ_ì»¬ëŸ¼ = None
            for col in df.columns:
                if "ì¶œê¸ˆ" in str(col):
                    ì¶œê¸ˆ_ì»¬ëŸ¼ = col
                    break
            
            if ì¶œê¸ˆ_ì»¬ëŸ¼ is None:
                raise ValueError("ì¶œê¸ˆê¸ˆì•¡ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            df = df[df[ì¶œê¸ˆ_ì»¬ëŸ¼].astype(str).str.strip() != ""]
            df = df[df[ì¶œê¸ˆ_ì»¬ëŸ¼].astype(str).str.strip() != "0"]
            
            # ìƒí˜¸ëª… ì¶”ì¶œ
            ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ = None
            for col in df.columns:
                if "ê±°ë˜ê¸°ë¡ì‚¬í•­" in str(col) or "ìƒí˜¸ëª…" in str(col):
                    ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ = col
                    break
            
            if ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ is None:
                # ê±°ë˜ë‚´ìš© ì»¬ëŸ¼ìœ¼ë¡œ ëŒ€ì²´
                for col in df.columns:
                    if "ê±°ë˜ë‚´ìš©" in str(col):
                        ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ = col
                        break
            
            if ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ is None:
                raise ValueError("ìƒí˜¸ëª… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            df["store_name"] = df[ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼].astype(str).str.strip()
            
            # ğŸ”¥ ê³ ê¸‰ AI ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ ì ìš©
            print(f"ğŸ¤– {'ê³ ê¸‰ AI' if self.ai_model_available else 'ê¸°ë³¸'} ë¶„ë¥˜ ëª¨ë“œë¡œ {len(df)}ê±´ ì²˜ë¦¬ ì¤‘...")
            df["predicted_category"] = df["store_name"].apply(self.categorize_store)
            
            # ê²°ê³¼ ìƒì„±
            result = []
            ê±°ë˜ì¼ì‹œ_ì»¬ëŸ¼ = None
            for col in df.columns:
                if "ê±°ë˜ì¼ì‹œ" in str(col) or "ê±°ë˜ì¼" in str(col):
                    ê±°ë˜ì¼ì‹œ_ì»¬ëŸ¼ = col
                    break
            
            for _, row in df.iterrows():
                try:
                    # ë‚ ì§œ íŒŒì‹±
                    transaction_date = datetime.now()
                    if ê±°ë˜ì¼ì‹œ_ì»¬ëŸ¼ and pd.notna(row[ê±°ë˜ì¼ì‹œ_ì»¬ëŸ¼]):
                        try:
                            transaction_date = pd.to_datetime(row[ê±°ë˜ì¼ì‹œ_ì»¬ëŸ¼])
                        except:
                            pass
                    
                    # ê¸ˆì•¡ íŒŒì‹±
                    amount_str = str(row[ì¶œê¸ˆ_ì»¬ëŸ¼]).replace(",", "").replace(" ", "")
                    amount = 0
                    try:
                        amount = float(re.sub(r'[^\d.]', '', amount_str))
                    except:
                        continue
                    
                    if amount <= 0:
                        continue
                    
                    result.append({
                        "transaction_date": transaction_date.isoformat() if hasattr(transaction_date, 'isoformat') else str(transaction_date),
                        "amount": amount,
                        "store_name": row["store_name"],
                        "category": row["predicted_category"],
                        "description": str(row[ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼]) if ê±°ë˜ê¸°ë¡ì‚¬í•­_ì»¬ëŸ¼ else "",
                        "payment_method": "ì¹´ë“œ"
                    })
                    
                except Exception as e:
                    print(f"í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… ì´ {len(result)}ê±´ì˜ ê±°ë˜ë¥¼ {'ê³ ê¸‰ AI' if self.ai_model_available else 'ê¸°ë³¸'}ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤.")
            return result
            
        except Exception as e:
            raise Exception(f"NH ê±°ë˜ë‚´ì—­ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")

    def parse_csv_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ì¼ë°˜ CSV ê±°ë˜ë‚´ì—­ íŒŒì¼ íŒŒì‹± (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´)
        """
        # ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
        if not self.model_initialized:
            self.initialize_model()
            
        try:
            # CSV íŒŒì¼ ì½ê¸° (ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„)
            encodings = ['utf-8', 'cp949', 'euc-kr']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    print(f"âœ… {encoding} ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì„±ê³µ")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("ì§€ì›í•˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"ğŸ“Š CSV íŒŒì¼ ì»¬ëŸ¼: {list(df.columns)}")
            print(f"ğŸ“Š ì´ {len(df)}í–‰ì˜ ë°ì´í„°")
            
            # ì»¬ëŸ¼ëª… ë§¤í•‘ (ë‹¤ì–‘í•œ í˜•íƒœì˜ ì»¬ëŸ¼ëª… ì§€ì›)
            date_columns = ['ë‚ ì§œ', 'ê±°ë˜ì¼', 'ê±°ë˜ì¼ì‹œ', 'date', 'ì¼ì']
            amount_columns = ['ê¸ˆì•¡', 'ì¶œê¸ˆì•¡', 'ì…ê¸ˆì•¡', 'ì¶œê¸ˆê¸ˆì•¡', 'ì…ê¸ˆê¸ˆì•¡', 'ì‚¬ìš©ê¸ˆì•¡', 'amount', 'ì§€ì¶œì•¡']
            merchant_columns = ['ê°€ë§¹ì ', 'ê°€ë§¹ì ëª…', 'ìƒí˜¸ëª…', 'ì ìš”', 'merchant', 'ê±°ë˜ì²˜', 'ë‚´ìš©']
            
            # ì‹¤ì œ ì»¬ëŸ¼ ì°¾ê¸°
            date_col = None
            amount_col = None
            merchant_col = None
            
            for col in df.columns:
                col_lower = str(col).lower()
                if not date_col:
                    for date_keyword in date_columns:
                        if date_keyword.lower() in col_lower:
                            date_col = col
                            break
                
                if not amount_col:
                    for amount_keyword in amount_columns:
                        if amount_keyword.lower() in col_lower:
                            amount_col = col
                            break
                
                if not merchant_col:
                    for merchant_keyword in merchant_columns:
                        if merchant_keyword.lower() in col_lower:
                            merchant_col = col
                            break
            
            print(f"ğŸ” ê°ì§€ëœ ì»¬ëŸ¼ - ë‚ ì§œ: {date_col}, ê¸ˆì•¡: {amount_col}, ê°€ë§¹ì : {merchant_col}")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if not amount_col:
                raise ValueError("ê¸ˆì•¡ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê¸ˆì•¡, ì¶œê¸ˆì•¡, ì‚¬ìš©ê¸ˆì•¡ ë“±)")
            
            if not merchant_col:
                raise ValueError("ê°€ë§¹ì  ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê°€ë§¹ì ëª…, ìƒí˜¸ëª…, ì ìš” ë“±)")
            
            # ë°ì´í„° ì²˜ë¦¬
            result = []
            
            # ğŸ”¥ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ê³ ê¸‰ AI ë¶„ë¥˜ (ì„±ëŠ¥ ìµœì í™”)
            valid_merchants = []
            valid_indices = []
            
            for index, row in df.iterrows():
                try:
                    # ê¸ˆì•¡ ê²€ì¦
                    amount_str = str(row[amount_col]).replace(",", "").replace(" ", "")
                    if pd.isna(row[amount_col]) or amount_str == '' or amount_str == 'nan':
                        continue
                    
                    amount = float(re.sub(r'[^\d.]', '', amount_str))
                    if amount <= 0:
                        continue
                    
                    # ê°€ë§¹ì ëª… ê²€ì¦
                    merchant_name = str(row[merchant_col]) if pd.notna(row[merchant_col]) else ""
                    if merchant_name.strip() == '' or merchant_name == 'nan':
                        merchant_name = "ê¸°íƒ€"
                    
                    valid_merchants.append(merchant_name)
                    valid_indices.append(index)
                    
                except:
                    continue
            
            # ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            print(f"ğŸ¤– {len(valid_merchants)}ê°œ ìƒí˜¸ëª…ì— ëŒ€í•´ {'ê³ ê¸‰ AI' if self.ai_model_available else 'ê¸°ë³¸'} ë¶„ë¥˜ ì§„í–‰...")
            categories = [self.categorize_store(merchant) for merchant in valid_merchants]
            
            # ê²°ê³¼ ìƒì„±
            for i, index in enumerate(valid_indices):
                try:
                    row = df.iloc[index]
                    
                    # ë‚ ì§œ íŒŒì‹±
                    transaction_date = datetime.now()
                    if date_col and pd.notna(row[date_col]):
                        try:
                            transaction_date = pd.to_datetime(row[date_col])
                        except:
                            pass
                    
                    # ê¸ˆì•¡ íŒŒì‹±
                    amount_str = str(row[amount_col]).replace(",", "").replace(" ", "")
                    amount = float(re.sub(r'[^\d.]', '', amount_str))
                    
                    result.append({
                        "transaction_date": transaction_date.isoformat() if hasattr(transaction_date, 'isoformat') else str(transaction_date),
                        "amount": amount,
                        "store_name": valid_merchants[i],
                        "category": categories[i],
                        "description": valid_merchants[i],
                        "payment_method": "ì¹´ë“œ"
                    })
                    
                except Exception as e:
                    print(f"í–‰ {index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… ì´ {len(result)}ê±´ì˜ ê±°ë˜ë¥¼ {'ê³ ê¸‰ AI' if self.ai_model_available else 'ê¸°ë³¸'}ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤.")
            return result
            
        except Exception as e:
            raise Exception(f"CSV íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
classification_service = TransactionClassificationService()